# -*- coding: utf-8 -*-
"""week7-hw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c_t5WjLBK4FRwZDKSwge_R8Sy1lhQV1M
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow
print(tensorflow.__version__)

"""# Practice"""

import time
import numpy as np
from matplotlib import pyplot as plt
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import RMSprop, SGD, Adam
from keras.datasets import mnist
from keras import regularizers
import keras.callbacks as CB
from keras.layers.normalization import BatchNormalization
import pandas as pd

from keras.utils import plot_model


def load_data():
    print ('Loading data...')
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')

    X_train /= 255
    X_test /= 255

    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)

    X_train = np.reshape(X_train, (60000, 784))
    X_test = np.reshape(X_test, (10000, 784))

    print ('Data loaded.')
    return [X_train, X_test, y_train, y_test]

####################################################################
## You can setup different networks with different regularizers here
#####################################################################
def init_model(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784, kernel_regularizer=regularizers.l2(0.001))) #Use L2
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dropout(0.4))   #Use dropout
    model.add(Dense(300, kernel_regularizer=regularizers.l1(0.001))) #Use L1
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dropout(0.4))
    model.add(Dense(10))
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model


def run_network(rms, data=None, model=None, epochs=20, batch=256):
    try:
        start_time = time.time()
        if data is None:
            X_train, X_test, y_train, y_test = load_data()
        else:
            X_train, X_test, y_train, y_test = data

        if model is None:
            model = init_model(rms)


        print ('Training model...')
        callbacks= [ CB.EarlyStopping(monitor='val_loss', patience=2, verbose=0) ] ## Use EarlyStop Here
        history = model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch,
                  validation_data=(X_test, y_test), callbacks=callbacks, verbose=2)

        print ("Training duration : {0}".format(time.time() - start_time))
        score = model.evaluate(X_test, y_test, batch_size=16)

        print ("Network's test score [loss, accuracy]: {0}".format(score))
        return model, history.history
    except KeyboardInterrupt:
        print (' KeyboardInterrupt')
        return model, history.history

def plot_losses(history_labels):
  for history, label in history_labels:
    plt.figure(1, figsize=(10,10)) 
    plt.subplot(211)   # 2 by 1 array of subplots, and draw the first one 
    plt.plot(history['loss'])  
    plt.plot(history['val_loss'])  
    plt.title('model loss ' + label)  
    plt.ylabel('loss')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'test'], loc='upper right')  
	
    plt.subplot(212)   # 2 by 1 array of subplots, and draw the second one 
    plt.plot(history['accuracy'])  
    plt.plot(history['val_accuracy'])  
    plt.title('model accuracy ' + label)  
    plt.ylabel('acc')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'test'], loc='lower right')  	
    plt.show() 
	
def plot_image(image):
    fig = plt.gcf() #design image size
    fig.set_size_inches(2,2) #design image size
    plt.imshow(image, cmap = 'binary') 
    plt.show()

def plot_images_labels_prediction(images, labels, prediction, idx, num=10):
    fig =plt.gcf()
    fig.set_size_inches(10,10)
    if num>25: num = 25
    for i in range(0,num):
        ax = plt.subplot(5,5,i+1) #Generate 5*5 subgraph
        ax.imshow(images[idx], cmap='binary')
        title = "label=" + str(labels[idx])
        if len(prediction)>0:
            title+=",predict=" + str(prediction[idx])
            ax.set_title(title, fontsize=10) #Setting subgraph title and size
            ax.set_xticks([]) #don't show the ticks
            ax.set_yticks([]) #don't show the ticks
            idx+=1
            plt.show()
	
#######################################################
## You can try different optimizers here by setting rms
########################################################
rms = SGD()
model1, hloss1 = run_network(rms)

rms = RMSprop()
model2, hloss2 = run_network(rms)


rms = Adam()
model3, hloss3 = run_network(rms)

#######################################################
## You can try different optimizers here by setting rms
########################################################
rms = SGD()
model1, hloss1 = run_network(rms)

rms = RMSprop()
model2, hloss2 = run_network(rms)


rms = Adam()
model3, hloss3 = run_network(rms)

labels = ['SGD', 'RMSprop', 'adam']
#######################################################
## You can try different optimizers here by setting rms
########################################################
rms = SGD()
model1, hloss1 = run_network(rms)

rms = RMSprop()
model2, hloss2 = run_network(rms)


rms = Adam()
model3, hloss3 = run_network(rms)

labels = ['SGD', 'RMSprop', 'adam']

plot_losses(zip([hloss1, hloss2, hloss3], labels) )	

## Test image with model1
(X_train, y_train), (X_test, y_test) = mnist.load_data()
Test_image = X_test.reshape(10000, 784).astype('float32')
i=1
for model in [model1, model2, model3]:
    print("=================================================")
    prediction = model.predict_classes(Test_image)
    print("顯示編號340之後10張: ")
    plot_images_labels_prediction(X_test, y_test, prediction, idx = 340) #內定測試10張圖片，最多25張
    print("顯示編號1379圖片預測季結果: ")
    plot_images_labels_prediction(X_test, y_test, prediction, 1378, 1) #測試1張圖片
    print("畫出模型: ")
    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model'+str(i)+'.png')
    print(model.summary())
    i+=1

plot_losses(zip([hloss1, hloss2, hloss3], labels) )

"""## 畫模型"""

from keras.utils import plot_model

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')
from IPython.display import Image
Image(retina=True, filename='model.png')

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
#Install plot model package
!apt install graphviz
!pip install pydot pydot-ng
!echo "Double check with Python 3"
!python -c "import pydot"

import time
import numpy as np
from matplotlib import pyplot as plt
from keras.utils import np_utils
import keras.callbacks as cb
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import RMSprop
from keras.datasets import mnist
import csv
from random import shuffle
import pandas as pd # 引用套件並縮寫為 pd  
import numpy as np
from keras.layers import Input, Dense
from keras.models import Model
from keras.utils import plot_model


def init_model():
    inputs = Input(shape=(64,))
    d = Dense(64, activation='relu')
    y1=d(inputs)
    y2= Dense(64, activation='relu')(y1)
    y3=d(y2)
    y4 = Dense(32, activation='sigmoid')(y3)
    model = Model(inputs=inputs, outputs=y4)
    rms = RMSprop()
    model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])

    return model



model = init_model()

# Model summary
print(model.summary())

# Plot model graph
plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')
from IPython.display import Image
Image(retina=True, filename='model.png')

"""# Answer

## 宣告
"""

import time
import numpy as np
from matplotlib import pyplot as plt
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import RMSprop, SGD, Adam
from keras.datasets import mnist
from keras import regularizers
import keras.callbacks as CB
from keras.layers.normalization import BatchNormalization

def load_data():
    print ('Loading data...')
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')

    X_train /= 255
    X_test /= 255

    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)

    X_train = np.reshape(X_train, (60000, 784))
    X_test = np.reshape(X_test, (10000, 784))

    print ('Data loaded.')
    return [X_train, X_test, y_train, y_test]


def run_network(rms, data=None, model=None, epochs=20, batch=256):
    print('=========================================================================================================')
    try:
        start_time = time.time()
        if data is None:
            X_train, X_test, y_train, y_test = load_data()
        else:
            X_train, X_test, y_train, y_test = data

        if model is None:
            model = init_model(rms)


        print ('Training model...')
        callbacks= [ CB.EarlyStopping(monitor='val_loss', patience=2, verbose=0) ] ## Use EarlyStop Here
        history = model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch,
                  validation_data=(X_test, y_test), callbacks=callbacks, verbose=2)

        print ("Training duration : {0}".format(time.time() - start_time))
        score = model.evaluate(X_test, y_test, batch_size=16)

        print ("Network's test score [loss, accuracy]: {0}".format(score))
        return model, history.history
    except KeyboardInterrupt:
        print (' KeyboardInterrupt')
        return model, history.history

def plot_losses(history_labels):
  for history, label in history_labels:
    plt.figure(1, figsize=(10,10)) 
    plt.subplot(211)   # 2 by 1 array of subplots, and draw the first one 
    plt.plot(history['loss'])  
    plt.plot(history['val_loss'])  
    plt.title('model loss ' + label)  
    plt.ylabel('loss')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'test'], loc='upper right')  
	
    plt.subplot(212)   # 2 by 1 array of subplots, and draw the second one 
    plt.plot(history['accuracy'])  
    plt.plot(history['val_accuracy'])  
    plt.title('model accuracy ' + label)  
    plt.ylabel('acc')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'test'], loc='lower right')  	
    plt.show() 
	
def plot_image(image):
    fig = plt.gcf() #design image size
    fig.set_size_inches(2,2) #design image size
    plt.imshow(image, cmap = 'binary') 
    plt.show()

def plot_images_labels_prediction(images, labels, prediction, idx, num=10):
    fig =plt.gcf()
    fig.set_size_inches(10,10)
    if num>25: num = 25
    for i in range(0,num):
        ax = plt.subplot(5,5,i+1) #Generate 5*5 subgraph
        ax.imshow(images[idx], cmap='binary')
        title = "label=" + str(labels[idx])
        if len(prediction)>0:
            title+=",predict=" + str(prediction[idx])
            ax.set_title(title, fontsize=10) #Setting subgraph title and size
            ax.set_xticks([]) #don't show the ticks
            ax.set_yticks([]) #don't show the ticks
            idx+=1
            plt.show()

####################################################################
## You can setup different networks with different regularizers here
#####################################################################
def init_model_L1(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784, kernel_regularizer=regularizers.l1(0.001))) #Use L1
    model.add(Activation('relu'))
    model.add(Dense(10))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model
  
def init_model_L2(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784, kernel_regularizer=regularizers.l2(0.001))) #Use L2
    model.add(Activation('relu'))
    model.add(Dense(10))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model

def init_model_Dropout(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784))
    model.add(Activation('relu'))
    model.add(Dropout(0.4))   #Use dropout
    model.add(Dense(10))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model

def init_model_Batch_Normalization_layers(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784))
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dense(10))
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model

def init_model_teacher(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784, kernel_regularizer=regularizers.l2(0.001))) #Use L2
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dropout(0.4))   #Use dropout
    model.add(Dense(300, kernel_regularizer=regularizers.l1(0.001))) #Use L1
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dropout(0.4))
    model.add(Dense(10))
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model

def init_model_nomal(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784)) 
    model.add(Dense(10))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model

rms = Adam()
model1 = init_model_teacher(rms)
model2 = init_model_nomal(rms)
model1, hloss1 = run_network(rms,model=model1)
model2, hloss2 = run_network(rms,model=model2)

labels = ['teacher', 'normal']
plot_losses(zip([hloss1, hloss2, hloss3,hloss4], labels) )

"""## SGD Optimizer"""

rms = SGD()
model1 = init_model_L1(rms)
model2 = init_model_L2(rms)
model3 = init_model_Dropout(rms)
model4 = init_model_Batch_Normalization_layers(rms)


model1, hloss1 = run_network(rms,model=model1)
model2, hloss2 = run_network(rms,model=model2)
model3, hloss3 = run_network(rms,model=model3)
model4, hloss4 = run_network(rms,model=model4)

labels = ['SGD+L1', 'SGD+L2', 'SGD+Dropout', 'SGD+BN']
plot_losses(zip([hloss1, hloss2, hloss3,hloss4], labels) )

"""## RMSprop Optimizer"""

rms = RMSprop()
model1 = init_model_L1(rms)
model2 = init_model_L2(rms)
model3 = init_model_Dropout(rms)
model4 = init_model_Batch_Normalization_layers(rms)


model1, hloss1 = run_network(rms,model=model1)
model2, hloss2 = run_network(rms,model=model2)
model3, hloss3 = run_network(rms,model=model3)
model4, hloss4 = run_network(rms,model=model4)

labels = ['RMSprop+L1', 'RMSprop+L2', 'RMSprop+Dropout', 'RMSprop+BN']
plot_losses(zip([hloss1, hloss2, hloss3,hloss4], labels) )

"""## Adam Optimizer"""

rms = Adam()
model1 = init_model_L1(rms)
model2 = init_model_L2(rms)
model3 = init_model_Dropout(rms)
model4 = init_model_Batch_Normalization_layers(rms)


model1, hloss1 = run_network(rms,model=model1)
model2, hloss2 = run_network(rms,model=model2)
model3, hloss3 = run_network(rms,model=model3)
model4, hloss4 = run_network(rms,model=model4)

labels = ['Adam+L1', 'Adam+L2', 'Adam+Dropout', 'Adam+BN']
plot_losses(zip([hloss1, hloss2, hloss3,hloss4], labels) )

"""## 其他"""

import time
import numpy as np
from matplotlib import pyplot as plt
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import RMSprop, SGD, Adam
from keras.datasets import mnist
from keras import regularizers
import keras.callbacks as CB
from keras.layers.normalization import BatchNormalization
import pandas as pd

def load_data():
    print ('Loading data...')
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')

    X_train /= 255
    X_test /= 255

    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)

    X_train = np.reshape(X_train, (60000, 784))
    X_test = np.reshape(X_test, (10000, 784))

    print ('Data loaded.')
    return [X_train, X_test, y_train, y_test]

####################################################################
## You can setup different networks with different regularizers here
#####################################################################
def init_model(rms):
    start_time = time.time()
    print ('Compiling Model ... ')
    model = Sequential()
    model.add(Dense(500, input_dim=784, kernel_regularizer=regularizers.l2(0.001))) #Use L2
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dropout(0.4))   #Use dropout
    model.add(Dense(300, kernel_regularizer=regularizers.l1(0.001))) #Use L1
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('relu'))
    model.add(Dropout(0.4))
    model.add(Dense(10))
    model.add(BatchNormalization())  #add a BN layer here
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
    print ('Model compield in {0} seconds'.format(time.time() - start_time))
    return model


def run_network(rms, data=None, model=None, epochs=20, batch=256):
    try:
        start_time = time.time()
        if data is None:
            X_train, X_test, y_train, y_test = load_data()
        else:
            X_train, X_test, y_train, y_test = data

        if model is None:
            model = init_model(rms)


        print ('Training model...')
        callbacks= [ CB.EarlyStopping(monitor='val_loss', patience=2, verbose=0) ] ## Use EarlyStop Here
        history = model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch,
                  validation_data=(X_test, y_test), callbacks=callbacks, verbose=2)

        print ("Training duration : {0}".format(time.time() - start_time))
        score = model.evaluate(X_test, y_test, batch_size=16)

        print ("Network's test score [loss, accuracy]: {0}".format(score))
        return model, history.history
    except KeyboardInterrupt:
        print (' KeyboardInterrupt')
        return model, history.history

def plot_losses(history_labels):
  for history, label in history_labels:
    plt.figure(1, figsize=(10,10)) 
    plt.subplot(211)   # 2 by 1 array of subplots, and draw the first one 
    plt.plot(history['loss'])  
    plt.plot(history['val_loss'])  
    plt.title('model loss ' + label)  
    plt.ylabel('loss')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'test'], loc='upper right')  
	
    plt.subplot(212)   # 2 by 1 array of subplots, and draw the second one 
    plt.plot(history['accuracy'])  
    plt.plot(history['val_accuracy'])  
    plt.title('model accuracy ' + label)  
    plt.ylabel('acc')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'test'], loc='lower right')  	
    plt.show() 
	
def plot_image(image):
    fig = plt.gcf() #design image size
    fig.set_size_inches(2,2) #design image size
    plt.imshow(image, cmap = 'binary') 
    plt.show()

def plot_images_labels_prediction(images, labels, prediction, idx, num=10):
    fig =plt.gcf()
    fig.set_size_inches(10,10)
    if num>25: num = 25
    for i in range(0,num):
        ax = plt.subplot(5,5,i+1) #Generate 5*5 subgraph
        ax.imshow(images[idx], cmap='binary')
        title = "label=" + str(labels[idx])
        if len(prediction)>0:
            title+=",predict=" + str(prediction[idx])
            ax.set_title(title, fontsize=10) #Setting subgraph title and size
            ax.set_xticks([]) #don't show the ticks
            ax.set_yticks([]) #don't show the ticks
            idx+=1
            plt.show()
	
#######################################################
## You can try different optimizers here by setting rms
########################################################
rms = SGD()
model1, hloss1 = run_network(rms)

rms = RMSprop()
model2, hloss2 = run_network(rms)


rms = Adam()
model3, hloss3 = run_network(rms)

labels = ['SGD', 'RMSprop', 'adam']

plot_losses(zip([hloss1, hloss2, hloss3], labels) )	

## Test image with model1
(X_train, y_train), (X_test, y_test) = mnist.load_data()
Test_image = X_test.reshape(10000, 784).astype('float32')
for model in [model1, model2, model3]:
    print("=================================================")
    prediction = model.predict_classes(Test_image)
    print("顯示編號340之後10張: ")
    plot_images_labels_prediction(X_test, y_test, prediction, idx = 340) #內定測試10張圖片，最多25張
    print("顯示編號1379圖片預測季結果: ")
    plot_images_labels_prediction(X_test, y_test, prediction, 1378, 1) #測試1張圖片
plot_losses(zip([hloss1, hloss2, hloss3], labels) )

