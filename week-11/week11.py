# -*- coding: utf-8 -*-
"""week11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ij0aRHXmKUdm-q1p-BPeyCUKHtwUBZnz
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow
print(tensorflow.__version__)
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('drive/My Drive/dataset/intel_small')

!ls

train_path = 'seg_train'
test_path = 'seg_test'
cat = os.listdir(train_path)
print(f'We have {len(cat)} classes: ', os.listdir(train_path))
# Examples of images
import matplotlib.pyplot as plt
import numpy as np
import cv2 as cv
rows, cols = (1, 5)

for c in cat:
    print(f'{c} images:')
    path = f'{train_path}/{c}'    
    fig = plt.figure(figsize = (13, 8))
    for i in range(rows * cols):
        fig.add_subplot(rows, cols, i+1)
        image_id = os.listdir(path)[np.random.randint(0, 100)]
        image = cv.imread(path + f'/{image_id}')
        plt.imshow(image[:, :, ::-1])
        plt.title(image_id)
        plt.axis('off')
    plt.show()

from keras.preprocessing.image import ImageDataGenerator
# Parameters of model
target_size = (150,150)
colormode = 'rgb'
seed = 666
batch_size = 1
input_shape = (150, 150)
reg = None # l2(0.0001)
axis = -1 # For BatchNormalization layers

# Creating ImageDataGenerator, which rescales images 
# and splits train data into train and validation sets 
# with 0.9 / 0.1 proportion
datagen = ImageDataGenerator(rescale = 1.0/255.0,
                            validation_split = 0.1
                            )

# Creating train, valid and test generators
train_generator = datagen.flow_from_directory(directory = train_path, # Path to directory which contains images classes
                                             target_size = target_size, # Whether resize images or not
                                             color_mode = colormode, 
                                             #batch_size = batch_size,
                                             class_mode = 'categorical',
                                             #shuffle = True,
                                             #seed = seed,
                                             subset = 'training') # Train or validation dataset

valid_generator = datagen.flow_from_directory(directory = train_path,
                                             target_size = target_size,
                                             color_mode = colormode,
                                             #batch_size = batch_size,
                                             class_mode = 'categorical',
                                             #shuffle = True,
                                             #seed = seed,
                                             subset = 'validation')

test_generator = datagen.flow_from_directory(directory = test_path,
                                            target_size = target_size,
                                            color_mode = colormode,
                                            batch_size = 1,
                                            class_mode = None,
                                            shuffle = False, 
                                            seed = seed)

# Define number of steps for fit_generator function
STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size

train_generator.image_shape

from keras.models import Sequential
from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization,Conv2D,MaxPooling2D
from keras.optimizers import RMSprop, SGD, Adam
import time
from keras.callbacks import ModelCheckpoint
def plot_losses(history_labels):
  for history, label in history_labels:
    plt.figure(1, figsize=(10,10)) 
    plt.subplot(211)   # 2 by 1 array of subplots, and draw the first one 
    plt.plot(history['loss'])  
    plt.plot(history['val_loss'])  
    plt.title('model loss ' + label)  
    plt.ylabel('loss')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'validation'], loc='upper right')  
	
    plt.subplot(212)   # 2 by 1 array of subplots, and draw the second one 
    plt.plot(history['accuracy'])  
    plt.plot(history['val_accuracy'])  
    plt.title('model accuracy ' + label)  
    plt.ylabel('acc')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'validation'], loc='lower right')  	
    plt.show()

from keras import regularizers
# Defining model architecture
model = Sequential()

model.add(Conv2D(32, (3, 3), input_shape=(150,150,3)))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3, 3)))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())

# Fully connected layer
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(Dense(6))

model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
model.summary()

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 30, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')

from keras import regularizers
# Defining model architecture
model2 = Sequential()

model2.add(Conv2D(128, (3, 3), input_shape=(150,150,3)))
model2.add(BatchNormalization(axis=-1))
model2.add(Activation('relu'))
model2.add(Conv2D(128, (3, 3)))
model2.add(BatchNormalization(axis=-1))
model2.add(Activation('relu'))
model2.add(MaxPooling2D(pool_size=(2,2)))

model2.add(Conv2D(256,(3, 3)))
model2.add(BatchNormalization(axis=-1))
model2.add(Activation('relu'))
model2.add(Conv2D(256, (3, 3)))
model2.add(BatchNormalization(axis=-1))
model2.add(Activation('relu'))
model2.add(MaxPooling2D(pool_size=(2,2)))


model2.add(Flatten())

# Fully connected layer
model2.add(Dense(512))
model2.add(BatchNormalization())
model2.add(Activation('relu'))
model2.add(Dropout(0.4))
model2.add(Dense(6))

model2.add(Activation('softmax'))

model2.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model2.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 200, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model2, show_shapes=True, show_layer_names=True, to_file='model.png')

from keras import regularizers
# Defining model architecture
model3 = Sequential()

model3.add(Conv2D(128, (3, 3), input_shape=(150,150,3)))
model3.add(BatchNormalization(axis=-1))
model3.add(Activation('relu'))
model3.add(Conv2D(128, (3, 3)))
model3.add(BatchNormalization(axis=-1))
model3.add(Activation('relu'))
model3.add(MaxPooling2D(pool_size=(2,2)))

model3.add(Conv2D(256,(3, 3)))
model3.add(BatchNormalization(axis=-1))
model3.add(Activation('relu'))
model3.add(Conv2D(256, (3, 3)))
model3.add(BatchNormalization(axis=-1))
model3.add(Activation('relu'))
model3.add(MaxPooling2D(pool_size=(2,2)))

model3.add(Conv2D(512,(3, 3)))
model3.add(BatchNormalization(axis=-1))
model3.add(Activation('relu'))
model3.add(Conv2D(512, (3, 3)))
model3.add(BatchNormalization(axis=-1))
model3.add(Activation('relu'))
model3.add(MaxPooling2D(pool_size=(2,2)))


model3.add(Flatten())

# Fully connected layer
model3.add(Dense(2048))
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))
model3.add(Dense(2048))
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))

model3.add(Dense(6))

model3.add(Activation('softmax'))

model3.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

from keras.callbacks import EarlyStopping
# Defining checkpoint c                                                                                                                                                                                                                                                        -+allback
checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model3.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 200, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model3, show_shapes=True, show_layer_names=True, to_file='model.png')

"""## VGG16"""

from keras import regularizers
# Defining model architecture
vgg16 = Sequential()

vgg16.add(Conv2D(64, (3, 3), input_shape=(150,150,3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(64, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16.add(Conv2D(128, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(128, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16.add(Conv2D(256, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(256, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(256, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16.add(Conv2D(512, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(512, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(512, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16.add(Conv2D(512, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(512, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(Conv2D(512, (3, 3),padding="same"))
vgg16.add(Activation('relu'))

vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16.add(Flatten())

# Fully connected layer
vgg16.add(Dense(4096))
vgg16.add(Activation('relu'))
vgg16.add(Dense(4096))
vgg16.add(Activation('relu'))
vgg16.add(Dense(6))

vgg16.add(Activation('softmax'))

vgg16.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = vgg16.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 200, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(vgg16, show_shapes=True, show_layer_names=True, to_file='model.png')

from keras import regularizers
# Defining model architecture
vgg16_new = Sequential()

vgg16_new.add(Conv2D(64, (3, 3), input_shape=(150,150,3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(64, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16_new.add(Conv2D(128, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(128, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16_new.add(Conv2D(256, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(256, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(256, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16_new.add(Conv2D(512, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(512, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(512, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16_new.add(Conv2D(512, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(512, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(Conv2D(512, (3, 3),padding="same"))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))

vgg16_new.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))

vgg16_new.add(Flatten())

# Fully connected layer
vgg16_new.add(Dense(4096))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))
vgg16_new.add(Dense(4096))
model.add(BatchNormalization(axis=-1))
vgg16_new.add(Activation('relu'))
vgg16_new.add(Dense(6))

vgg16_new.add(Activation('softmax'))

vgg16_new.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
vgg16_new.summary()

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = vgg16_new.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 200, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(vgg16_new, show_shapes=True, show_layer_names=True, to_file='model.png')

"""## Transfer learning"""

from keras.applications.vgg16 import VGG16
vgg16 = VGG16(include_top=False, input_shape=(150,150,3))
model = Sequential(vgg16.layers)
for layer in model.layers[:15]:
  layer.traninable = False
model.add(Flatten())
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(Dense(6))
model.add(Activation('sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model.summary())

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 200, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')

