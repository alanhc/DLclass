# -*- coding: utf-8 -*-
"""week8-10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_63e8kwZw38YhLbFAT5qWPX_XyluggUR
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow
print(tensorflow.__version__)

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('drive/My Drive/dataset/intel_small')

!ls

train_path = 'seg_train'
test_path = 'seg_test'
cat = os.listdir(train_path)
print(f'We have {len(cat)} classes: ', os.listdir(train_path))

# Examples of images
import matplotlib.pyplot as plt
import numpy as np
import cv2 as cv
rows, cols = (1, 5)

for c in cat:
    print(f'{c} images:')
    path = f'{train_path}/{c}'    
    fig = plt.figure(figsize = (13, 8))
    for i in range(rows * cols):
        fig.add_subplot(rows, cols, i+1)
        image_id = os.listdir(path)[np.random.randint(0, 100)]
        image = cv.imread(path + f'/{image_id}')
        plt.imshow(image[:, :, ::-1])
        plt.title(image_id)
        plt.axis('off')
    plt.show()

from keras.preprocessing.image import ImageDataGenerator
# Parameters of model
target_size = (28,28)
colormode = 'grayscale'
seed = 666
batch_size = 1
input_shape = (150, 150, 3)
reg = None # l2(0.0001)
axis = -1 # For BatchNormalization layers

# Creating ImageDataGenerator, which rescales images 
# and splits train data into train and validation sets 
# with 0.9 / 0.1 proportion
datagen = ImageDataGenerator(rescale = 1.0/255.0,
                            validation_split = 0.1
                            )

# Creating train, valid and test generators
train_generator = datagen.flow_from_directory(directory = train_path, # Path to directory which contains images classes
                                             target_size = target_size, # Whether resize images or not
                                             color_mode = colormode, 
                                             batch_size = batch_size,
                                             class_mode = 'categorical',
                                             shuffle = True,
                                             seed = seed,
                                             subset = 'training') # Train or validation dataset

valid_generator = datagen.flow_from_directory(directory = train_path,
                                             target_size = target_size,
                                             color_mode = colormode,
                                             batch_size = batch_size,
                                             class_mode = 'categorical',
                                             shuffle = True,
                                             seed = seed,
                                             subset = 'validation')

test_generator = datagen.flow_from_directory(directory = test_path,
                                            target_size = target_size,
                                            color_mode = colormode,
                                            batch_size = 1,
                                            class_mode = None,
                                            shuffle = False, 
                                            seed = seed)

# Define number of steps for fit_generator function
STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size

train_generator.image_shape

from keras.models import Sequential
from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization
from keras.optimizers import RMSprop, SGD, Adam
import time
from keras.callbacks import ModelCheckpoint

def plot_losses(history_labels):
  for history, label in history_labels:
    plt.figure(1, figsize=(10,10)) 
    plt.subplot(211)   # 2 by 1 array of subplots, and draw the first one 
    plt.plot(history['loss'])  
    plt.plot(history['val_loss'])  
    plt.title('model loss ' + label)  
    plt.ylabel('loss')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'validation'], loc='upper right')  
	
    plt.subplot(212)   # 2 by 1 array of subplots, and draw the second one 
    plt.plot(history['accuracy'])  
    plt.plot(history['val_accuracy'])  
    plt.title('model accuracy ' + label)  
    plt.ylabel('acc')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'validation'], loc='lower right')  	
    plt.show()

# Defining model architecture
momentum = 0.9
rms=Adam()
model = Sequential()
model.add(Dense(1000, input_shape=(28,28,1))) 
model.add(Flatten())
model.add(Activation('relu'))
model.add(Dense(100))
model.add(Activation('relu'))
model.add(Dense(6))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])

# Defining checkpoint callback
checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)

# Fit model
import time
start_time = time.time()
history = model.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 30, callbacks = [checkpoint])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history

from keras.utils import plot_model
labels = ['test']
plot_losses(zip([history], labels) )	
plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')

from keras import regularizers
# Defining model architecture
rms=Adam()
model2 = Sequential()
model2.add(Dense(1000, input_shape=(28,28,1),kernel_regularizer=regularizers.l2(0.01))) 
model2.add(Flatten())
model2.add(Activation('relu'))
model2.add(Dropout(0.4))
model2.add(Dense(100))
model2.add(Activation('relu'))
model2.add(Dense(6))
model2.add(Activation('softmax'))
model2.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model2.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 30, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history

from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	
plot_model(model2, show_shapes=True, show_layer_names=True, to_file='model.png')



from keras import regularizers
# Defining model architecture
rms=Adam()
model3 = Sequential()
model3.add(Dense(1000, input_shape=(28,28,1),kernel_regularizer=regularizers.l2(0.001))) 
model3.add(Flatten())
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))

model3.add(Dense(100,kernel_regularizer=regularizers.l2(0.001)))
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))

model3.add(Dense(100,kernel_regularizer=regularizers.l2(0.001)))
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))

model3.add(Dense(100,kernel_regularizer=regularizers.l2(0.001)))
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))

model3.add(Dense(100,kernel_regularizer=regularizers.l2(0.001)))
model3.add(BatchNormalization())
model3.add(Activation('relu'))
model3.add(Dropout(0.4))

model3.add(Dense(6))

model3.add(Activation('softmax'))
model3.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model3.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 30, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model3, show_shapes=True, show_layer_names=True, to_file='model.png')

