{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwYDI1uCALl",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.imgur.com/Cm0u08v.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akj_yq0zTUza",
        "colab_type": "code",
        "outputId": "7c93b4c9-518b-46a6-f798-62286683359f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGEdTUVlWx-F",
        "colab_type": "text"
      },
      "source": [
        "# 定義模型\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXt3CwyLU1zO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Sequential([\n",
        "  Dense(32, input_shape=(784,)),\n",
        "  Activation('relu'),\n",
        "  Dense(10),\n",
        "  Activation('softmax'),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLiJo3SuWMCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(500, input_dim=784))\n",
        "model2.add(Activation('relu')) \n",
        "model2.add(Dense(300))\n",
        "model2.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUDi5oBFWo8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile模型\n",
        "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CEeSdnLdzzt",
        "colab_type": "code",
        "outputId": "ed958f0c-16b4-4d72-d768-4d2c7a6e179c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "def load_data():\n",
        "    print ('Loading data...')\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train, 10)\n",
        "    y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "    X_train = np.reshape(X_train, (60000, 784))\n",
        "    X_test = np.reshape(X_test, (10000, 784))\n",
        "\n",
        "    print ('Data loaded.')\n",
        "    return [X_train, X_test, y_train, y_test]\n",
        "X_train, X_test, y_train, y_test = load_data()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw7ecByKe7DQ",
        "colab_type": "code",
        "outputId": "b7ea3ffa-afd1-464c-e914-21c20fd29ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "import keras.callbacks as cb\n",
        "class LossHistory(cb.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        batch_loss = logs.get('loss')\n",
        "        self.losses.append(batch_loss)\n",
        "\n",
        "\n",
        "epochs=10\n",
        "batch=32\n",
        "history = LossHistory()\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch,\n",
        "                    callbacks=[history],\n",
        "                    validation_data=(X_test, y_test), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " - 7s - loss: 1.8471e-04 - acc: 0.9999 - val_loss: 0.1306 - val_acc: 0.9820\n",
            "Epoch 2/10\n",
            " - 7s - loss: 6.8646e-04 - acc: 0.9997 - val_loss: 0.1319 - val_acc: 0.9821\n",
            "Epoch 3/10\n",
            " - 7s - loss: 6.5233e-04 - acc: 0.9998 - val_loss: 0.1336 - val_acc: 0.9820\n",
            "Epoch 4/10\n",
            " - 7s - loss: 4.1664e-04 - acc: 0.9999 - val_loss: 0.1314 - val_acc: 0.9821\n",
            "Epoch 5/10\n",
            " - 7s - loss: 5.6306e-04 - acc: 0.9998 - val_loss: 0.1341 - val_acc: 0.9809\n",
            "Epoch 6/10\n",
            " - 7s - loss: 2.6880e-04 - acc: 0.9999 - val_loss: 0.1433 - val_acc: 0.9824\n",
            "Epoch 7/10\n",
            " - 7s - loss: 2.5914e-04 - acc: 0.9999 - val_loss: 0.1584 - val_acc: 0.9812\n",
            "Epoch 8/10\n",
            " - 7s - loss: 2.1159e-04 - acc: 1.0000 - val_loss: 0.1420 - val_acc: 0.9825\n",
            "Epoch 9/10\n",
            " - 7s - loss: 2.7246e-04 - acc: 0.9999 - val_loss: 0.1477 - val_acc: 0.9814\n",
            "Epoch 10/10\n",
            " - 7s - loss: 9.4206e-05 - acc: 0.9999 - val_loss: 0.1400 - val_acc: 0.9829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f888c21b470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BB-yO_2cexg",
        "colab_type": "code",
        "outputId": "ef01415a-8335-4259-87b6-7621d0b22c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "# 模型訓練\n",
        "model.fit(X_train, y_train, epoch=epochs, batch_size=batch,\n",
        "                  callbacks=[history], # 畫圖\n",
        "                  validation_data=(X_test, y_test), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1ccb275f24b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, y_train, epoch=epochs, batch_size=batch,\n\u001b[1;32m      2\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 畫圖\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                   validation_data=(X_test, y_test), verbose=2)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nb_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unrecognized keyword arguments: {'epoch': 10}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyHBd671hHMe",
        "colab_type": "text"
      },
      "source": [
        "# 寫成 class 的 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3pUexqhWJeS",
        "colab_type": "code",
        "outputId": "20205bfa-1545-433b-b870-05c10d072748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "import keras.callbacks as cb\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "class LossHistory(cb.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        batch_loss = logs.get('loss')\n",
        "        self.losses.append(batch_loss)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    print ('Loading data...')\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train, 10)\n",
        "    y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "    X_train = np.reshape(X_train, (60000, 784))\n",
        "    X_test = np.reshape(X_test, (10000, 784))\n",
        "\n",
        "    print ('Data loaded.')\n",
        "    return [X_train, X_test, y_train, y_test]\n",
        "\n",
        "\n",
        "def init_model():\n",
        "    start_time = time.time()\n",
        "    print ('Compiling Model ... ')\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=784)) \n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    rms = RMSprop()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "    print ('Model compield in {0} seconds'.format(time.time() - start_time))\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_network(data=None, model=None, epochs=20, batch=256):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if data is None:\n",
        "            X_train, X_test, y_train, y_test = load_data()\n",
        "        else:\n",
        "            X_train, X_test, y_train, y_test = data\n",
        "\n",
        "        if model is None:\n",
        "            model = init_model()\n",
        "\n",
        "        history = LossHistory()\n",
        "\n",
        "        print ('Training model...')\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch,\n",
        "                    callbacks=[history],\n",
        "                    validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "        print (\"Training duration : {0}\".format(time.time() - start_time))\n",
        "        score = model.evaluate(X_test, y_test, batch_size=16)\n",
        "\n",
        "        print (\"Network's test score [loss, accuracy]: {0}\".format(score))\n",
        "        return model, history.losses\n",
        "    except KeyboardInterrupt:\n",
        "        print (' KeyboardInterrupt')\n",
        "        return model, history.losses\n",
        "\n",
        "\n",
        "def plot_losses(losses):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(losses)\n",
        "    ax.set_title('Loss per batch')\n",
        "    fig.show()\n",
        "\n",
        "model, losses = run_network(epochs=50, batch=256)\n",
        "plot_losses(losses)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data loaded.\n",
            "Compiling Model ... \n",
            "Model compield in 0.06328582763671875 seconds\n",
            "Training model...\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 9s - loss: 0.3079 - acc: 0.9116 - val_loss: 0.1617 - val_acc: 0.9509\n",
            "Epoch 2/50\n",
            " - 1s - loss: 0.1309 - acc: 0.9613 - val_loss: 0.1065 - val_acc: 0.9687\n",
            "Epoch 3/50\n",
            " - 1s - loss: 0.0869 - acc: 0.9746 - val_loss: 0.0966 - val_acc: 0.9706\n",
            "Epoch 4/50\n",
            " - 1s - loss: 0.0631 - acc: 0.9814 - val_loss: 0.0821 - val_acc: 0.9743\n",
            "Epoch 5/50\n",
            " - 1s - loss: 0.0485 - acc: 0.9855 - val_loss: 0.0661 - val_acc: 0.9791\n",
            "Epoch 6/50\n",
            " - 1s - loss: 0.0373 - acc: 0.9891 - val_loss: 0.0692 - val_acc: 0.9797\n",
            "Epoch 7/50\n",
            " - 1s - loss: 0.0295 - acc: 0.9914 - val_loss: 0.0632 - val_acc: 0.9807\n",
            "Epoch 8/50\n",
            " - 1s - loss: 0.0231 - acc: 0.9935 - val_loss: 0.0744 - val_acc: 0.9784\n",
            "Epoch 9/50\n",
            " - 1s - loss: 0.0186 - acc: 0.9948 - val_loss: 0.0636 - val_acc: 0.9808\n",
            "Epoch 10/50\n",
            " - 1s - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0628 - val_acc: 0.9818\n",
            "Epoch 11/50\n",
            " - 1s - loss: 0.0117 - acc: 0.9969 - val_loss: 0.0671 - val_acc: 0.9798\n",
            "Epoch 12/50\n",
            " - 1s - loss: 0.0090 - acc: 0.9980 - val_loss: 0.0675 - val_acc: 0.9809\n",
            "Epoch 13/50\n",
            " - 1s - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0641 - val_acc: 0.9820\n",
            "Epoch 14/50\n",
            " - 1s - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0699 - val_acc: 0.9820\n",
            "Epoch 15/50\n",
            " - 1s - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0739 - val_acc: 0.9825\n",
            "Epoch 16/50\n",
            " - 1s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.0807 - val_acc: 0.9809\n",
            "Epoch 17/50\n",
            " - 1s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0864 - val_acc: 0.9791\n",
            "Epoch 18/50\n",
            " - 1s - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0807 - val_acc: 0.9811\n",
            "Epoch 19/50\n",
            " - 1s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9821\n",
            "Epoch 20/50\n",
            " - 1s - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0811 - val_acc: 0.9822\n",
            "Epoch 21/50\n",
            " - 1s - loss: 9.8982e-04 - acc: 0.9998 - val_loss: 0.0911 - val_acc: 0.9813\n",
            "Epoch 22/50\n",
            " - 1s - loss: 8.8825e-04 - acc: 0.9999 - val_loss: 0.0823 - val_acc: 0.9828\n",
            "Epoch 23/50\n",
            " - 1s - loss: 8.7157e-04 - acc: 0.9997 - val_loss: 0.0835 - val_acc: 0.9833\n",
            "Epoch 24/50\n",
            " - 1s - loss: 6.8970e-04 - acc: 0.9998 - val_loss: 0.0875 - val_acc: 0.9829\n",
            "Epoch 25/50\n",
            " - 1s - loss: 5.3502e-04 - acc: 0.9998 - val_loss: 0.0832 - val_acc: 0.9831\n",
            "Epoch 26/50\n",
            " - 1s - loss: 3.8556e-04 - acc: 0.9999 - val_loss: 0.0898 - val_acc: 0.9829\n",
            "Epoch 27/50\n",
            " - 1s - loss: 2.9280e-04 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 0.9830\n",
            "Epoch 28/50\n",
            " - 1s - loss: 2.3674e-04 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9825\n",
            "Epoch 29/50\n",
            " - 1s - loss: 2.9932e-04 - acc: 0.9999 - val_loss: 0.0923 - val_acc: 0.9820\n",
            "Epoch 30/50\n",
            " - 1s - loss: 3.6964e-04 - acc: 0.9999 - val_loss: 0.0948 - val_acc: 0.9830\n",
            "Epoch 31/50\n",
            " - 1s - loss: 1.3407e-04 - acc: 1.0000 - val_loss: 0.0991 - val_acc: 0.9830\n",
            "Epoch 32/50\n",
            " - 1s - loss: 1.3552e-04 - acc: 1.0000 - val_loss: 0.1036 - val_acc: 0.9823\n",
            "Epoch 33/50\n",
            " - 1s - loss: 1.0325e-04 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 0.9818\n",
            "Epoch 34/50\n",
            " - 1s - loss: 7.6227e-05 - acc: 1.0000 - val_loss: 0.1032 - val_acc: 0.9836\n",
            "Epoch 35/50\n",
            " - 1s - loss: 4.5438e-05 - acc: 1.0000 - val_loss: 0.1040 - val_acc: 0.9829\n",
            "Epoch 36/50\n",
            " - 1s - loss: 3.7964e-05 - acc: 1.0000 - val_loss: 0.1031 - val_acc: 0.9834\n",
            "Epoch 37/50\n",
            " - 1s - loss: 8.6167e-05 - acc: 1.0000 - val_loss: 0.1142 - val_acc: 0.9816\n",
            "Epoch 38/50\n",
            " - 1s - loss: 4.9057e-05 - acc: 1.0000 - val_loss: 0.1110 - val_acc: 0.9820\n",
            "Epoch 39/50\n",
            " - 1s - loss: 9.8079e-05 - acc: 1.0000 - val_loss: 0.1092 - val_acc: 0.9830\n",
            "Epoch 40/50\n",
            " - 1s - loss: 4.6990e-05 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9829\n",
            "Epoch 41/50\n",
            " - 1s - loss: 2.3644e-05 - acc: 1.0000 - val_loss: 0.1106 - val_acc: 0.9828\n",
            "Epoch 42/50\n",
            " - 1s - loss: 6.2173e-05 - acc: 1.0000 - val_loss: 0.1186 - val_acc: 0.9816\n",
            "Epoch 43/50\n",
            " - 1s - loss: 5.5645e-05 - acc: 1.0000 - val_loss: 0.1123 - val_acc: 0.9818\n",
            "Epoch 44/50\n",
            " - 1s - loss: 5.7754e-06 - acc: 1.0000 - val_loss: 0.1130 - val_acc: 0.9830\n",
            "Epoch 45/50\n",
            " - 1s - loss: 1.4329e-05 - acc: 1.0000 - val_loss: 0.1110 - val_acc: 0.9815\n",
            "Epoch 46/50\n",
            " - 1s - loss: 4.0650e-06 - acc: 1.0000 - val_loss: 0.1161 - val_acc: 0.9824\n",
            "Epoch 47/50\n",
            " - 1s - loss: 3.8531e-05 - acc: 1.0000 - val_loss: 0.1124 - val_acc: 0.9837\n",
            "Epoch 48/50\n",
            " - 1s - loss: 2.9828e-06 - acc: 1.0000 - val_loss: 0.1128 - val_acc: 0.9838\n",
            "Epoch 49/50\n",
            " - 1s - loss: 2.8271e-07 - acc: 1.0000 - val_loss: 0.1128 - val_acc: 0.9835\n",
            "Epoch 50/50\n",
            " - 1s - loss: 2.4146e-07 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.9834\n",
            "Training duration : 60.40389394760132\n",
            "10000/10000 [==============================] - 1s 95us/step\n",
            "Network's test score [loss, accuracy]: [0.11396803218169647, 0.9834]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEICAYAAAC+iFRkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB2T0yDOpxRYTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}