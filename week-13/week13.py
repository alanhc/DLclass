# -*- coding: utf-8 -*-
"""week13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0IbrFZRHTOFMAB2ltHakdR3RignnlNW

https://keras.io/api/applications/
https://keras.io/zh/applications/
https://towardsdatascience.com/deep-learning-using-transfer-learning-python-code-for-resnet50-8acdfb3a2d38
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow
print(tensorflow.__version__)
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('drive/My Drive/dataset/intel_small')

train_path = 'seg_train'
test_path = 'seg_test'
cat = os.listdir(train_path)
print(f'We have {len(cat)} classes: ', os.listdir(train_path))
# Examples of images
import matplotlib.pyplot as plt
import numpy as np
import cv2 as cv
rows, cols = (1, 5)

for c in cat:
    print(f'{c} images:')
    path = f'{train_path}/{c}'    
    fig = plt.figure(figsize = (13, 8))
    for i in range(rows * cols):
        fig.add_subplot(rows, cols, i+1)
        image_id = os.listdir(path)[np.random.randint(0, 100)]
        image = cv.imread(path + f'/{image_id}')
        plt.imshow(image[:, :, ::-1])
        plt.title(image_id)
        plt.axis('off')
    plt.show()
from keras.preprocessing.image import ImageDataGenerator
# Parameters of model
target_size = (150,150)
colormode = 'rgb'
seed = 666
batch_size = 1
input_shape = (150, 150)
reg = None # l2(0.0001)
axis = -1 # For BatchNormalization layers

# Creating ImageDataGenerator, which rescales images 
# and splits train data into train and validation sets 
# with 0.9 / 0.1 proportion
datagen = ImageDataGenerator(rescale = 1.0/255.0,
                            validation_split = 0.1
                            )

# Creating train, valid and test generators
train_generator = datagen.flow_from_directory(directory = train_path, # Path to directory which contains images classes
                                             target_size = target_size, # Whether resize images or not
                                             color_mode = colormode, 
                                             #batch_size = batch_size,
                                             class_mode = 'categorical',
                                             #shuffle = True,
                                             #seed = seed,
                                             subset = 'training') # Train or validation dataset

valid_generator = datagen.flow_from_directory(directory = train_path,
                                             target_size = target_size,
                                             color_mode = colormode,
                                             #batch_size = batch_size,
                                             class_mode = 'categorical',
                                             #shuffle = True,
                                             #seed = seed,
                                             subset = 'validation')

test_generator = datagen.flow_from_directory(directory = test_path,
                                            target_size = target_size,
                                            color_mode = colormode,
                                            batch_size = 1,
                                            class_mode = None,
                                            shuffle = False, 
                                            seed = seed)

# Define number of steps for fit_generator function
STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size

from keras.models import Sequential
from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization,Conv2D,MaxPooling2D
from keras.optimizers import RMSprop, SGD, Adam
import time
from keras.callbacks import ModelCheckpoint
from tensorflow.keras import regularizers
def plot_losses(history_labels):
  for history, label in history_labels:
    plt.figure(1, figsize=(10,10)) 
    plt.subplot(211)   # 2 by 1 array of subplots, and draw the first one 
    plt.plot(history['loss'])  
    plt.plot(history['val_loss'])  
    plt.title('model loss ' + label)  
    plt.ylabel('loss')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'validation'], loc='upper right')  
	
    plt.subplot(212)   # 2 by 1 array of subplots, and draw the second one 
    plt.plot(history['accuracy'])  
    plt.plot(history['val_accuracy'])  
    plt.title('model accuracy ' + label)  
    plt.ylabel('acc')  
    plt.xlabel('epoch')  
    plt.legend(['train', 'validation'], loc='lower right')  	
    plt.show()

from keras.applications.vgg16 import VGG16
vgg16 = VGG16(include_top=False, input_shape=(150,150,3))
model = Sequential(vgg16.layers)


for layer in model.layers[:15]:
  layer.trainable = False


model.add(Flatten())
model.add(Dense(256,activation='relu',kernel_regularizer=regularizers.l2(0.1)))
model.add(Dropout(0.4))
model.add(Dense(6,activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model.summary())

import pandas as pd
layers = [(layer, layer.name, layer.trainable) for layer in model.layers]

pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 100, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model 
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')

"""# VGG19"""

from keras.applications.vgg19 import VGG19
vgg19 = VGG19(include_top=False, input_shape=(150,150,3))
model2 = Sequential(vgg19.layers)
for layer in model2.layers[:-3]:
  layer.trainable = False
import pandas as pd
layers = [(layer, layer.name, layer.trainable) for layer in model2.layers]

model2.add(Flatten())
model2.add(Dense(256))
model2.add(Activation('relu'))
model2.add(Dropout(0.4))
model2.add(Dense(6))
model2.add(Activation('softmax'))
model2.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model2.summary())

pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model2.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 100, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model 
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model2, show_shapes=True, show_layer_names=True, to_file='model.png')

"""# ResNet50"""

from keras.applications.resnet50 import ResNet50
from keras.models import Sequential
from keras.models import Model
from keras.optimizers import RMSprop, SGD, Adam

restnet = ResNet50(include_top=False, input_shape=(150,150,3), weights='imagenet')
output = restnet.layers[-1].output
output = Flatten()(output)
restnet = Model(restnet.input, output=output)

for layer in restnet.layers[:5]:
  layer.trainable = True
for layer in restnet.layers[5:]:
  layer.trainable = False

import pandas as pd
layers = [(layer, layer.name, layer.trainable) for layer in restnet.layers]

model3 = Sequential()
model3.add(restnet)
model3.add(Dense(256, activation='relu', input_dim=input_shape))
#model3.add(Dropout(0.4))
model3.add(Dense(6, activation='softmax'))


model3.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model3.summary())

pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model3.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 100, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model 
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model3, show_shapes=True, show_layer_names=True, to_file='model.png')

"""# inceptionv3"""

from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization,Conv2D,MaxPooling2D
from keras.models import Sequential
from keras.models import Model
from keras.optimizers import RMSprop, SGD, Adam

from keras.applications.inception_v3 import InceptionV3
input_shape = (150,150,3)
inceptionV3 = InceptionV3(include_top=False, weights='imagenet', input_shape=(150,150,3))
output = inceptionV3.layers[-1].output
output = Flatten()(output)
inceptionV3 = Model(inceptionV3.input, output=output)

for layer in inceptionV3.layers[:-3]:
  layer.trainable = False

import pandas as pd
layers = [(layer, layer.name, layer.trainable) for layer in inceptionV3.layers]


model4 = Sequential()
model4.add(inceptionV3)
model4.add(Dense(256, activation='relu', input_dim=input_shape))
model4.add(Dropout(0.4))
model4.add(Dense(6, activation='softmax'))


model4.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model4.summary())
pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model4.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 100, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model 
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model4, show_shapes=True, show_layer_names=True, to_file='model.png')

"""# InceptionResNetV2"""

from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization,Conv2D,MaxPooling2D
from keras.models import Sequential
from keras.models import Model
from keras.optimizers import RMSprop, SGD, Adam

from keras.applications.inception_resnet_v2 import InceptionResNetV2
input_shape = (150,150,3)
inceptionResNetV2 = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(150,150,3))
output = inceptionResNetV2.layers[-1].output
output = Flatten()(output)
inceptionResNetV2 = Model(inceptionResNetV2.input, output=output)
for layer in inceptionResNetV2.layers[:-3]:
  layer.trainable = False

import pandas as pd
layers = [(layer, layer.name, layer.trainable) for layer in inceptionResNetV2.layers]

model5 = Sequential()
model5.add(inceptionResNetV2)
model5.add(Dense(256, activation='relu', input_dim=input_shape))
model5.add(Dropout(0.4))
model5.add(Dense(6, activation='softmax'))


model5.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model5.summary())
pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model5.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 100, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model 
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model5, show_shapes=True, show_layer_names=True, to_file='model.png')

"""# MobileNet"""

from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization,Conv2D,MaxPooling2D
from keras.models import Sequential
from keras.models import Model
from keras.optimizers import RMSprop, SGD, Adam

from keras.applications.mobilenet import MobileNet
input_shape = (150,150,3)
mobileNet = MobileNet(include_top=False, weights='imagenet', input_shape=(150,150,3))
output = mobileNet.layers[-1].output
output = Flatten()(output)
mobileNet = Model(mobileNet.input, output=output)
for layer in mobileNet.layers[:-3]:
  layer.trainable = False
import pandas as pd
layers = [(layer, layer.name, layer.trainable) for layer in mobileNet.layers]


model6 = Sequential()
model6.add(inceptionV3)
model6.add(Dense(256, activation='relu', input_dim=input_shape))
model6.add(Dropout(0.4))
model6.add(Dense(6, activation='softmax'))


model6.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print(model6.summary())
pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])

from keras.callbacks import EarlyStopping
# Defining checkpoint callback
#checkpoint = ModelCheckpoint('../model_save/call_back_history.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)
es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)
# Fit model
import time
start_time = time.time()
history = model6.fit_generator(generator = train_generator ,
                             steps_per_epoch = STEP_SIZE_TRAIN,
                             validation_data = valid_generator,
                             validation_steps = STEP_SIZE_VALID,
                             epochs = 100, callbacks = [es])
print ("Training duration : {0}".format(time.time() - start_time))  
history=history.history
from keras.utils import plot_model 
labels = ['test']  
plot_losses(zip([history], labels) )	

plot_model(model6, show_shapes=True, show_layer_names=True, to_file='model.png')

"""# Feature Extraction"""

# example of using the vgg16 model as a feature extraction model
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
from keras.models import Model
from pickle import dump
# load an image from file
image = load_img('dog.jpg', target_size=(224, 224))
# convert the image pixels to a numpy array
image = img_to_array(image)
# reshape data for the model
image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
# prepare the image for the VGG model
image = preprocess_input(image)
# load model
model = VGG16()
# remove the output layer
model.layers.pop()
model = Model(inputs=model.inputs, outputs=model.layers[-1].output)
# get extracted features
features = model.predict(image)
print(features.shape)
# save to file
dump(features, open('dog.pkl', 'wb'))

import matplotlib.pylab as plt

print(train_generator.next()[0][0].shape)
plt.imshow(train_generator.next()[0][0])